{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 551 Assignment 1 : Getting Started With Machine Learning\n",
    "\n",
    "### K- Nearest Neighbors Experiments\n",
    "\n",
    "#### Group 1: Rudi Kischer, Ben Hepditch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- make sure to install the requirements.txt file, and to use the correct virtual environment with juptyer notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Dataset 1: NHANES age prediction.csv (National Health and Nutrition Health Sur- vey 2013-2014 (NHANES) Age Prediction Subset): https://archive.ics.uci.edu/dataset/887/national+health+and+nutrition+health+survey+2013-2014+(nhanes)+age+prediction+subset\n",
    "\n",
    "Dataset 2: Breast Cancer Wisconsin (Original) dataset: https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # DATASET 1: NHANES age prediction.csv\n",
    "national_health_and_nutrition_health_survey_2013_2014_nhanes_age_prediction_subset = fetch_ucirepo(id=887) \n",
    "dataset_1 = national_health_and_nutrition_health_survey_2013_2014_nhanes_age_prediction_subset.data\n",
    "X_1 = dataset_1.features \n",
    "y_1 = dataset_1.targets \n",
    "\n",
    "# # DATASET 2: Breast Cancer Wisconsin\n",
    "breast_cancer_wisconsin_original = fetch_ucirepo(id=15) \n",
    "dataset_2 = breast_cancer_wisconsin_original.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data\n",
    "\n",
    "- We want to remove all rows from our data sets which have null values in the targets or in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Cleaning Function\n",
    "def clean(dataset):\n",
    "  X = dataset.features\n",
    "  Y = dataset.targets\n",
    "  missing_rows_features = X.isnull().any(axis=1)\n",
    "  missing_rows_targets = Y.isnull().any(axis=1)\n",
    "  missing_rows = missing_rows_features | missing_rows_targets\n",
    "  \n",
    "  print(f\"features_missing: {missing_rows_features.sum()}\")\n",
    "  print(f\"targets_missing: {missing_rows_targets.sum()}\")\n",
    "\n",
    "  X_clean = X[-missing_rows]\n",
    "  Y_clean = Y[-missing_rows]\n",
    "  print(f'{missing_rows.sum()} rows deleted')\n",
    "  dataset.features = X_clean\n",
    "  dataset.targets = Y_clean\n",
    "\n",
    "  return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_missing: 0\n",
      "targets_missing: 0\n",
      "0 rows deleted\n",
      "features_missing: 16\n",
      "targets_missing: 0\n",
      "16 rows deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean the DataSets\n",
    "dataset_1 = clean(dataset_1)\n",
    "dataset_2 = clean(dataset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Statistics\n",
    "\n",
    "- We want to get some statistics about our target values. We want to know the mean and the squared difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mean\n",
    "def grouped_target_means(dataset):\n",
    "    # grouped by the target\n",
    "    X = dataset.features\n",
    "    Y = dataset.targets\n",
    "\n",
    "    XY = pd.concat([X,Y], axis=1)\n",
    "    XY_grouped = XY.groupby(Y.columns[0])\n",
    "    XY_mean = XY_grouped.mean()\n",
    "    return XY_mean\n",
    "\n",
    "# Define Feature Distance\n",
    "def grouped_feature_distance(dataset):\n",
    "    XY_mean = grouped_target_means(dataset)\n",
    "\n",
    "    sqr_diff = (XY_mean.iloc[0] - XY_mean.iloc[1]) ** 2\n",
    "    df_sqr_diff = pd.DataFrame([sqr_diff], index=['squarred_diff'])\n",
    "    return df_sqr_diff\n",
    "\n",
    "# Print Col Ranking\n",
    "def feature_ranking(dataset):\n",
    "    df_sqr_diff = grouped_feature_distance(dataset)\n",
    "    row = df_sqr_diff.iloc[0]\n",
    "    sorted_row = row.sort_values(ascending=False)\n",
    "\n",
    "    ranking_df = pd.DataFrame({\n",
    "      'Feature': sorted_row.index,\n",
    "      'Value': sorted_row.values,\n",
    "      'Rank': range(1, len(sorted_row) + 1)\n",
    "    })\n",
    "\n",
    "    return ranking_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 Feature Means:\n",
      "           RIAGENDR  PAQ605  BMXBMI   LBXGLU  DIQ010   LBXGLT   LBXIN\n",
      "age_group                                                            \n",
      "Adult         1.512   1.806  27.968   98.645   2.014  109.991  12.107\n",
      "Senior        1.508   1.909  27.886  104.330   2.027  141.209  10.405\n",
      "Dataset 2 Feature Means: \n",
      "       Clump_thickness  Uniformity_of_cell_size  Uniformity_of_cell_shape  Marginal_adhesion  Single_epithelial_cell_size  Bare_nuclei  Bland_chromatin  Normal_nucleoli  Mitoses\n",
      "Class                                                                                                                                                                            \n",
      "2                2.964                    1.306                     1.414              1.347                        2.108        1.347            2.083            1.261    1.065\n",
      "4                7.188                    6.577                     6.561              5.586                        5.326        7.628            5.975            5.858    2.603\n"
     ]
    }
   ],
   "source": [
    "# Get grouped means\n",
    "print(f'Dataset 1 Feature Means:')\n",
    "XY_1_bar = grouped_target_means(dataset_1)\n",
    "print(XY_1_bar)\n",
    "\n",
    "print(f'Dataset 2 Feature Means: ')\n",
    "XY_2_bar = grouped_target_means(dataset_2)\n",
    "print(XY_2_bar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group Feature Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "                RIAGENDR  PAQ605  BMXBMI  LBXGLU     DIQ010   LBXGLT  LBXIN\n",
      "squarred_diff  1.425e-05   0.011   0.007  32.319  1.786e-04  974.576  2.895\n",
      "Dataset 2:\n",
      "               Clump_thickness  Uniformity_of_cell_size  Uniformity_of_cell_shape  Marginal_adhesion  Single_epithelial_cell_size  Bare_nuclei  Bland_chromatin  Normal_nucleoli  Mitoses\n",
      "squarred_diff           17.845                   27.784                    26.484             17.969                       10.357       39.448           15.144           21.128    2.363\n"
     ]
    }
   ],
   "source": [
    "print('Dataset 1:')\n",
    "XY_1_fd = grouped_feature_distance(dataset_1)\n",
    "print(XY_1_fd)\n",
    "\n",
    "print('Dataset 2:')\n",
    "XY_2_fd = grouped_feature_distance(dataset_2)\n",
    "print(XY_2_fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features Ranked By Squared Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 Feature Ranking\n",
      "    Feature      Value  Rank\n",
      "0    LBXGLT  9.746e+02     1\n",
      "1    LBXGLU  3.232e+01     2\n",
      "2     LBXIN  2.895e+00     3\n",
      "3    PAQ605  1.065e-02     4\n",
      "4    BMXBMI  6.728e-03     5\n",
      "5    DIQ010  1.786e-04     6\n",
      "6  RIAGENDR  1.425e-05     7\n",
      "Dataset 1 Feature Ranking\n",
      "                       Feature   Value  Rank\n",
      "0                  Bare_nuclei  39.448     1\n",
      "1      Uniformity_of_cell_size  27.784     2\n",
      "2     Uniformity_of_cell_shape  26.484     3\n",
      "3              Normal_nucleoli  21.128     4\n",
      "4            Marginal_adhesion  17.969     5\n",
      "5              Clump_thickness  17.845     6\n",
      "6              Bland_chromatin  15.144     7\n",
      "7  Single_epithelial_cell_size  10.357     8\n",
      "8                      Mitoses   2.363     9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Dataset 1 Feature Ranking\")\n",
    "d1_feature_ranking = feature_ranking(dataset_1)\n",
    "print(d1_feature_ranking)\n",
    "\n",
    "print(\"Dataset 2 Feature Ranking\")\n",
    "d2_feature_ranking = feature_ranking(dataset_2)\n",
    "print(d2_feature_ranking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  TODO: *Description goes here analyzing if the features that are strongly different are associated with the target*\n",
    "\n",
    "- Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k nearest neighbour uses a data set to predict the classification of a new data point. The algorithm works by finding the k nearest neighbours to the new data point and classifying the new data point as the most common classification of the k nearest neighbours. The algorithm can be used for both classification and regression problems. For classification problems, the algorithm uses the most common classification of the k nearest neighbours. For regression problems, the algorithm uses the average of the k nearest neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "#x1 : series of a df\n",
    "#x2 : series of a df\n",
    "\n",
    "def euclidean(x1, x2):\n",
    "  return np.linalg.norm(x1 - x2)\n",
    "\n",
    "def manhattan(x1,x2):\n",
    "  return (x1 - x2).abs().sum()\n",
    "\n",
    "def chebyshev(x1, x2):\n",
    "  return (x1 - x2).abs().max()\n",
    "\n",
    "def hamming(x1, x2):\n",
    "  return (~(x1 == x2)).sum()\n",
    "\n",
    "def cosineSim(x1, x2):\n",
    "  prod = x1.dot(x2)\n",
    "  mag = np.linalg.norm(x1) * np.linalg.norm(x2)\n",
    "  return prod / mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K nearest Neighbour\n",
    "# Mostly following the design from :\n",
    "# https://github.com/yueliyl/comp551-notebooks/blob/master/KNN.ipynb\n",
    "\n",
    "class KNN():\n",
    "\n",
    "  def __init__(self, k=1, similiarity_fn=None):\n",
    "    if not similiarity_fn:\n",
    "      similiarity_fn = euclidean\n",
    "    self.similiarity_fn = similiarity_fn\n",
    "    self.k = k\n",
    "  \n",
    "  def standardize_features(self):\n",
    "    x_d_bar = self.X.mean()\n",
    "    x_d_sd = self.X.std(ddof=0)\n",
    "    self.X = (self.X - x_d_bar) / x_d_sd\n",
    "\n",
    "  def fit(self, X, Y, k):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "    self.k = k\n",
    "    # self.C = self.Y[:, 0].nunique()\n",
    "    # self.standardize_features()\n",
    "\n",
    "  def predict_sample(self, x_i):\n",
    "    similiarity_scores = self.X.apply(lambda x_j: self.similiarity_fn(x_i,x_j), axis=1)\n",
    "    top_k_idx = similiarity_scores.nsmallest(self.k).index \n",
    "    top_k_neighbour_labels =  self.Y.loc[top_k_idx][self.Y.columns[0]]\n",
    "    labels = self.Y.iloc[:, 0].unique()\n",
    "    label_probs = top_k_neighbour_labels.value_counts(normalize=True).reindex(labels, fill_value=0)\n",
    "    return pd.Series(label_probs, index=labels)\n",
    "  \n",
    "  def predict(self, x):\n",
    "    prob_df = x.apply(self.predict_sample, axis=1)\n",
    "    y = prob_df.idxmax(axis=1)\n",
    "    predicted_probabilities = prob_df\n",
    "    y_df = pd.DataFrame(y, columns=self.Y.columns)\n",
    "    return y_df, predicted_probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(dataset, split, shuffle=True, seed=1):\n",
    "    X = dataset.features\n",
    "    Y = dataset.targets\n",
    "    XY = pd.concat([X, Y], axis=1)\n",
    "    XY_shuffled = XY.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    split_idx = int(len(XY) * split)\n",
    "    \n",
    "    y_size = -Y.shape[1]\n",
    "\n",
    "    # Train Data Set\n",
    "    train_set = XY_shuffled[split_idx:]\n",
    "    X_train = train_set.iloc[:, :y_size]\n",
    "    Y_train = train_set.iloc[:, y_size:]\n",
    "\n",
    "    # Test Data Set\n",
    "    test_set = XY_shuffled[:split_idx]\n",
    "    X_test = test_set.iloc[:, :y_size]\n",
    "    Y_test = test_set.iloc[:, y_size:]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "\n",
    "X1_train, Y1_train, X1_test, Y1_test = train_test_split(dataset_1, test_size)\n",
    "\n",
    "X2_train, Y2_train, X2_test, Y2_test = train_test_split(dataset_1, test_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN()\n",
    "\n",
    "knn.fit(X1_train, Y1_train, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, label_probs = knn.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age_group'], dtype='object')\n",
      "Prediction\n",
      "    age_group\n",
      "0       Adult\n",
      "1       Adult\n",
      "2       Adult\n",
      "3       Adult\n",
      "4       Adult\n",
      "..        ...\n",
      "222     Adult\n",
      "223     Adult\n",
      "224     Adult\n",
      "225     Adult\n",
      "226    Senior\n",
      "\n",
      "[227 rows x 1 columns]\n",
      "Correct Labels\n",
      "    age_group\n",
      "0       Adult\n",
      "1       Adult\n",
      "2       Adult\n",
      "3      Senior\n",
      "4       Adult\n",
      "..        ...\n",
      "222     Adult\n",
      "223     Adult\n",
      "224     Adult\n",
      "225     Adult\n",
      "226     Adult\n",
      "\n",
      "[227 rows x 1 columns]\n",
      "Number of Correct Predictions\n",
      "227\n",
      "227\n",
      "Percentage of Correct Predictions: 85.90308370044053%\n"
     ]
    }
   ],
   "source": [
    "print(prediction.columns)\n",
    "print('Prediction')\n",
    "print(prediction)\n",
    "\n",
    "print('Correct Labels')\n",
    "print(Y1_test)\n",
    "\n",
    "# print the number of correct predictions\n",
    "print('Number of Correct Predictions')\n",
    "\n",
    "# align operands\n",
    "# prediction, Y1_test = prediction.align(Y1_test, join='inner')\n",
    "\n",
    "print(len(prediction))\n",
    "print(len(Y1_test))\n",
    "\n",
    "correct = (prediction == Y1_test)[Y1_test.columns[0]].sum()\n",
    "# percentage of correct predictions\n",
    "print(f'Percentage of Correct Predictions: {correct / len(prediction) * 100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
